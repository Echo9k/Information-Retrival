{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PA7_part4_WebCrawler.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN70780JTBprC6mKhJNlM+l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Echo9k/Information-Retrival/blob/main/PA7_part4_WebCrawler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CwxaWNMtr_w"
      },
      "source": [
        "# Web crawler\r\n",
        "Information retrieval.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNdnO3YRuMqn"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\r\n",
        "  <td>\r\n",
        "    <a target=\"_blank\" href=\"https://rebrand.ly/PA7_part4_WebCrawler_colab\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\r\n",
        "  </td>\r\n",
        "  <td>\r\n",
        "    <a target=\"_blank\" href=\"https://rebrand.ly/PA7_part4_WebCrawler_git\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\r\n",
        "  </td>\r\n",
        "  <td>\r\n",
        "    <a href=\"https://rebrand.ly/PA7_part4_WebCrawler_notebook\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\r\n",
        "\r\n",
        "  </td>\r\n",
        "  <td>\r\n",
        "    <a target=\"_blank\" href=\"https://rebrand.ly/PA7_part4_WebCrawler_py\"><img src=\"https://miro.medium.com/max/256/1*ztqS5rRI29GHxZa6uPF2UA.png\" width=\"40\" height=\"40\" />Download .py</a>\r\n",
        "  </td>\r\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5A-KE_IdhTY"
      },
      "source": [
        "## Set up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "T9bpCF8TTPY3"
      },
      "source": [
        "#@markdown ### Import libriaries\n",
        "import re\n",
        "import tqdm\n",
        "import time\n",
        "\n",
        "import sys\n",
        "\n",
        "\n",
        "from urllib.request import urlopen \n",
        "from urllib.error import URLError\n",
        "from bs4 import BeautifulSoup \n",
        "import pandas as pd"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "52CZZ5Z4TTMT"
      },
      "source": [
        "#@title Class Crawler\r\n",
        "# crawler\r\n",
        "class Crawler():\r\n",
        "    def __init__(self, domain, queue_limit=500):\r\n",
        "        self.domain = domain\r\n",
        "        self.queue=[domain]\r\n",
        "        self.queue_limit = queue_limit\r\n",
        "        self.info = {}  # Text just for the first few sites.\r\n",
        "    \r\n",
        "    def make_soup(self, URL)->BeautifulSoup:\r\n",
        "        try:\r\n",
        "            html = urlopen(URL)\r\n",
        "            return BeautifulSoup(html, 'html.parser')\r\n",
        "        except (TimeoutError, URLError) as err:\r\n",
        "            print(f\"error: {err} on {URL}\")\r\n",
        "            raise\r\n",
        "\r\n",
        "    def get_urls(self, URL, include_external=False)-> (set, str):\r\n",
        "        soup=self.make_soup(URL)\r\n",
        "        regx_internal= re.compile('^(?!#)((?!\\/\\/).)*$')  # Incomplete links are internal\r\n",
        "        regx_full_URL= re.compile('^(ht)(.)*$')\r\n",
        "\r\n",
        "        internal=set([self.domain+link.attrs['href'] for link in soup.find_all('a', href=regx_internal) if 'href' in link.attrs])\r\n",
        "        if include_external:\r\n",
        "            full_URLs=set([link.attrs['href'] for link in soup.find_all('a', href=regx_full_URL) if 'href' in link.attrs])\r\n",
        "            links = internal.union(full_URLs)\r\n",
        "        else:\r\n",
        "            links = internal\r\n",
        "        \r\n",
        "        return links, soup.text\r\n",
        "\r\n",
        "    def queue_maker(self, queue_limit=500, quiet=False):\r\n",
        "        while len(self.queue)<=queue_limit:\r\n",
        "            URL = self.queue.pop(0)\r\n",
        "            try: links, text = self.get_urls(URL)\r\n",
        "            except (TimeoutError, URLError) as err: continue\r\n",
        "            except: print(f\"Unexpected error: {sys.exc_info()[0]}  while visiting {URL}\"); continue\r\n",
        "            self.info.update({URL: text})\r\n",
        "            if links is not None:\r\n",
        "                new_unique_URLs=set(self.queue).union(self.info)\\\r\n",
        "                                .symmetric_difference(links).intersection(links)\r\n",
        "                self.queue.extend(new_unique_URLs)\r\n",
        "                if not quiet: print(f\"Queue size: {len(self.queue):<7} {len(new_unique_URLs):> 3} new unique URLs from  {URL: <7}\")\r\n",
        "        return self.queue, self.info\r\n",
        "\r\n",
        "    def extractor(self):\r\n",
        "        \r\n",
        "        for i, URL in enumerate(self.queue,10):\r\n",
        "            try:soup=self.make_soup(URL)\r\n",
        "            except: print(f\"Unexpected error: {sys.exc_info()[0]}  while visiting {URL}\"); continue\r\n",
        "            self.info.update({URL: soup.get_text(strip=True)})\r\n",
        "            if i%100==0: print(f\"{i}%\")\r\n",
        "            sys.stdout.write(\"-\")\r\n",
        "            sys.stdout.flush()"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGsH40IrjfaE"
      },
      "source": [
        "# Extract the info"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "u5bGWrBxaJH-"
      },
      "source": [
        "domain='http://www.hometownlife.com'  #@param{type:\"string\"}\r\n",
        "limit_queue = 500 #@param {type:\"slider\", min:50, max:550, step:50}\r\n",
        "crawler = Crawler(domain)\r\n",
        "crawler.queue_maker(limit_queue)\r\n",
        "print(\"extracting...\")\r\n",
        "crawler.extractor()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "yyZlooqwfQy0",
        "outputId": "3809ecb3-655e-4df1-d444-3304b634275c"
      },
      "source": [
        "#@markdown Display data\r\n",
        "%load_ext google.colab.data_table\r\n",
        "data = pd.DataFrame(pd.Series(crawler.info))\r\n",
        "data.reset_index(inplace=True)\r\n",
        "data.columns=[\"url\",\"text\"]\r\n",
        "# data.to_csv(\"data.csv\")\r\n",
        "data"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The google.colab.data_table extension is already loaded. To reload it, use:\n",
            "  %reload_ext google.colab.data_table\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAcp_N6ulLUn"
      },
      "source": [
        "# Statistics\r\n",
        "Hello pears! I was planning into making a package out of the python Class from the [ notebook previous](https://colab.research.google.com/drive/14AV2jstGnxw67HaK7o6tZfAun0zVGx_s) but I could not replicate what I did on my computer here on colab.\r\n",
        "\r\n",
        "Yet, this are the metrics I obtained.\r\n",
        "|metric|value|\r\n",
        "|:---|----|\r\n",
        "Number of documents processed | 527\r\n",
        "Total number of terms parsed from all documents | 1715640\r\n",
        "Total number of unique terms found and added to the index | 35911\r\n",
        "Total number of terms found that matched one of the stop words in your programâ€™s stop words list | 263241"
      ]
    }
  ]
}